{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import string\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_in = \"../data/semi_supervised_splits/train.json\"\n",
    "val_data_in = \"../data/semi_supervised_splits/val.json\"\n",
    "test_data_in = \"../data/test.json\"\n",
    "\n",
    "train_data_out = \"../preprocessed_data/semi_superviesd/preprocessed_train.pkl\"\n",
    "val_data_out = \"../preprocessed_data/semi_superviesd/preprocessed_val.pkl\"\n",
    "test_data_out = \"../preprocessed_data/semi_superviesd/preprocessed_test.pkl\"\n",
    "\n",
    "glove_in = \"/Users/gpsbhargav/projects/glove.840B.300d.txt\"\n",
    "glove_out = \"../preprocessed_data/semi_superviesd/embedding_matrix\"\n",
    "\n",
    "max_seq_len = 250\n",
    "\n",
    "pad_symbol = \"<pad>\"\n",
    "unk_symbol = \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickler(pkl_file, obj):\n",
    "    with open(pkl_file, \"wb\") as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def unpickler(pkl_file):\n",
    "    with open(pkl_file, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \n",
    "    def __init__(self, unk = '<unk>', pad='<pad>', other_special_symbols=None):\n",
    "        self.vocab = Counter([])\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "        self.min_word_count = 2\n",
    "        self.unk = unk\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.word_to_id[pad] = 0\n",
    "        self.word_to_id[unk] = 1\n",
    "        \n",
    "        self.id_to_word[0] = pad\n",
    "        self.id_to_word[1] = unk\n",
    "        \n",
    "        if(other_special_symbols is not None):\n",
    "            for i,symbol in enumerate(other_special_symbols):\n",
    "                self.id_to_word[len(self.id_to_word)] = symbol\n",
    "                self.word_to_id[symbol] = len(self.word_to_id)\n",
    "        \n",
    "        \n",
    "    def fit(self,text):\n",
    "        self.vocab.update(text)\n",
    "    \n",
    "    def freeze_vocab(self, min_word_count = 5):\n",
    "        self.min_word_count = min_word_count\n",
    "        sorted_counts = sorted(self.vocab.items(), key=lambda x: x[1], reverse = True)\n",
    "        sorted_counts_filtered = [item for item in sorted_counts if item[1] >= self.min_word_count]\n",
    "        for i, item in enumerate(sorted_counts_filtered):\n",
    "            if(item[0] not in self.word_to_id.keys()):\n",
    "                self.id_to_word[len(self.id_to_word)] = item[0]\n",
    "                self.word_to_id[item[0]] = len(self.word_to_id)\n",
    "            \n",
    "    \n",
    "    def transform_sent(self, text):\n",
    "        return [self.word_to_id.get(item, self.word_to_id[self.unk]) for item in text]\n",
    "    \n",
    "    def batch_transform(self, text_list):\n",
    "        out = []\n",
    "        for text in text_list:\n",
    "            out.append(self.transform_sent(text))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open(file_path) as in_file:\n",
    "        contents = json.load(in_file)\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = re.sub(\n",
    "            r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", \n",
    "            str(text))\n",
    "    text = re.sub(r\"[ ]+\", \" \", text)\n",
    "    text = re.sub(r\"\\!+\", \"!\", text)\n",
    "    text = re.sub(r\"\\,+\", \",\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "    text = text.lower().strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [x.text for x in nlp.tokenizer(normalize(text)) if x.text != \" \"]\n",
    "\n",
    "# def tokenize(text):\n",
    "#     return [x.text for x in nlp.tokenizer(text) if x.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'i', 'ca', \"n't\", 'fly', 'in', 'case', 'you', 'did', \"n't\", 'know', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(\"Hello world! I can't fly (In case you didn't know).\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_json(train_data_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'text', 'label'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = read_json(val_data_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = read_json(test_data_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(unk = unk_symbol, pad=pad_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:44<00:00, 444.64it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(training_data):\n",
    "    text = item[\"text\"]\n",
    "    text_tokenized = tokenize(text)\n",
    "    vocab.fit(text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80769"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.freeze_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27574"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"Hello world! I can't fly (In case you didn't know).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'i', 'ca', \"n't\", 'fly', 'in', 'case', 'you', 'did', \"n't\", 'know', '.']\n"
     ]
    }
   ],
   "source": [
    "test_text_tokenized = tokenize(test_text)\n",
    "print(test_text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4682, 186, 13, 185, 28, 2400, 12, 430, 27, 80, 28, 130, 4]\n"
     ]
    }
   ],
   "source": [
    "test_text_ids = vocab.transform_sent(test_text_tokenized)\n",
    "print(test_text_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_test_text = []\n",
    "for idx in test_text_ids:\n",
    "    word = vocab.id_to_word[idx]\n",
    "    reconstructed_test_text.append(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'i', 'ca', \"n't\", 'fly', 'in', 'case', 'you', 'did', \"n't\", 'know', '.']\n"
     ]
    }
   ],
   "source": [
    "print(reconstructed_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = vocab.word_to_id[pad_symbol]\n",
    "unk_id = vocab.word_to_id[unk_symbol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(pad_id)\n",
    "print(unk_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trim(sequence, max_len, pad_id):\n",
    "    ''' Puts padding before actual data '''\n",
    "    seq = sequence[:max_len]\n",
    "    mask = [1] * len(seq)\n",
    "    seq = [pad_id] * (max_len - len(seq)) + seq\n",
    "    mask = [pad_id] * (max_len - len(mask)) + mask\n",
    "    assert len(seq) == len(mask)\n",
    "    return seq, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 1, 2, 3], [0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_trim(sequence=[1,2,3], max_len=5, pad_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_example(data, vocab, pad_id, max_seq_len):\n",
    "    ''' \n",
    "    Outputs:\n",
    "    {\n",
    "        \"id\": example_id\n",
    "        \"text\": tokenized text,\n",
    "        \"label\": either 0 or 1. 0 is negative, 1 is positive,\n",
    "        \"word_mask\": binary vector. 0 denotes padding. 1 denotes given words\n",
    "        \"num_tokens\": number of tokens present in text\n",
    "    }\n",
    "    '''\n",
    "    out_data = {}\n",
    "    text_raw = data[\"text\"]\n",
    "    text_tokenized = tokenize(text_raw)\n",
    "    text_ids = vocab.transform_sent(text_tokenized)\n",
    "    text_ids_fixed_len, mask = pad_trim(sequence=text_ids, max_len=max_seq_len, \n",
    "                                        pad_id=pad_id)\n",
    "    num_tokens = sum(mask)\n",
    "    label = 0 if data[\"label\"] == 'neg' else 1\n",
    "    if(data[\"label\"] == 'neg'):\n",
    "        label = 0\n",
    "    elif(data[\"label\"] == 'pos'):\n",
    "        label = 1\n",
    "    else:\n",
    "        label = -1\n",
    "    \n",
    "    out_data[\"id\"] = data[\"id\"]\n",
    "    out_data[\"text\"] = text_ids_fixed_len\n",
    "    out_data[\"word_mask\"] = mask\n",
    "    out_data[\"label\"] = label\n",
    "    out_data[\"num_tokens\"] = num_tokens\n",
    "    \n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_example = {\"id\": 123, \"text\": \"Hello world! I still can't fly.\", \"label\":'neg' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test_example = process_one_example(data=text_example, vocab=vocab, \n",
    "                    pad_id=pad_id, max_seq_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'text', 'word_mask', 'label', 'num_tokens'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_test_example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "0\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(processed_test_example[\"id\"])\n",
    "print(processed_test_example[\"label\"])\n",
    "print(processed_test_example[\"num_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 4682, 186, 13, 135, 185, 28, 2400, 4]\n"
     ]
    }
   ],
   "source": [
    "print(processed_test_example[\"text\"][-15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(processed_test_example[\"word_mask\"][-15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_data(all_data, vocab, pad_id, max_seq_len):\n",
    "    out_list = []\n",
    "    for data in tqdm(all_data):\n",
    "        processed_data = process_one_example(data=data, vocab=vocab, \n",
    "                    pad_id=pad_id, max_seq_len=max_seq_len)\n",
    "        out_list.append(processed_data)\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:46<00:00, 434.28it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_training_data = process_all_data(training_data, vocab, pad_id, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(processed_training_data) == len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:10<00:00, 472.04it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_val_data = process_all_data(val_data, vocab, pad_id, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(processed_val_data) == len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:52<00:00, 472.19it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_test_data = process_all_data(test_data, vocab, pad_id, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(processed_test_data) == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickler(pkl_file=train_data_out, obj=processed_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickler(pkl_file=val_data_out, obj=processed_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickler(pkl_file=test_data_out, obj=processed_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare GloVe embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [03:23, 10796.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read GloVe file\n",
      "Preparing embedding matrix\n",
      "Embedding matrix shape:  (27574, 300)\n",
      "Number of words not found in GloVe:  1979\n",
      "Number of words in GloVe:  2196016\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 300\n",
    "VOCAB_SIZE = len(vocab.word_to_id)\n",
    "embeddings_index = {}\n",
    "f = open(glove_in,encoding='utf8')\n",
    "for line in tqdm(f):\n",
    "      values = line.split(' ')\n",
    "      word = values[0]\n",
    "      coefs = np.asarray(values[1:], dtype='float32')\n",
    "      embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(\"Read GloVe file\")\n",
    "\n",
    "# make sure GloVE doesn't have <unk> and <pad>.  NOTE: These will be handled separately later\n",
    "# assert(embeddings_index.get('<pad>',-10) == -10)\n",
    "# assert(embeddings_index.get('<unk>',-10) == -10)\n",
    "\n",
    "\n",
    "# prepare embedding matrix\n",
    "print(\"Preparing embedding matrix\")\n",
    "count_not_found = 0\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_SIZE))\n",
    "for word, i in vocab.word_to_id.items():\n",
    "    if((word == '<unk>') or (word == '<pad>')):\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        count_not_found += 1\n",
    "\n",
    "# initialize <unk> to mean of all embeddings\n",
    "embedding_matrix[vocab.word_to_id['<unk>']] = embedding_matrix.mean(axis = 0)\n",
    "\n",
    "print(\"Embedding matrix shape: \",embedding_matrix.shape)  \n",
    "print(\"Number of words not found in GloVe: \",count_not_found)\n",
    "print(\"Number of words in GloVe: \", len(embeddings_index))\n",
    "np.save(glove_out, embedding_matrix)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
